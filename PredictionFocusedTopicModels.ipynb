{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rzUedFYVAa_p"
   },
   "outputs": [],
   "source": [
    "# if working on google colab\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "aUn8gxC7_9_Y"
   },
   "outputs": [],
   "source": [
    "#@title Imports and device\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np \n",
    "import pickle\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import torch.distributions as ds\n",
    "import sklearn.model_selection as ms\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from opt_einsum import contract\n",
    "from sklearn.metrics import roc_auc_score as auc\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "NTlUZY3MANcQ"
   },
   "outputs": [],
   "source": [
    "#@title Utils\n",
    "def batchify(to_batch, batch_size):\n",
    "  M = to_batch[0].shape[0]\n",
    "  rand = torch.randperm(M)\n",
    "  for thing in to_batch:\n",
    "    thing = thing[rand]\n",
    "  \n",
    "  i = 0\n",
    "  out = [[] for thing in to_batch]\n",
    "\n",
    "  while i + batch_size < M:\n",
    "    for j, thing in enumerate(to_batch):\n",
    "      out[j].append(thing[i:i+batch_size])\n",
    "    i += batch_size\n",
    "  for j, thing in enumerate(to_batch):\n",
    "      out[j].append(thing[i:])\n",
    "  return out\n",
    "\n",
    "def coherence_single(w1, w2, W):\n",
    "  eps = 0.01\n",
    "  dw1 = (W[:, w1] > 0)\n",
    "  dw2 = (W[:, w2] > 0)\n",
    "  N = W.shape[0]\n",
    "\n",
    "  dw1w2 = (dw1 & dw2).float().sum() / N + eps\n",
    "  dw1 = dw1.float().sum() / N + eps\n",
    "  dw2 = dw2.float().sum() / N + eps\n",
    "\n",
    "  return dw1w2.log() - dw1.log() - dw2.log()\n",
    " \n",
    "def coherence(topics, W):\n",
    "  score = 0\n",
    "  count = 0\n",
    "  K, V = topics.shape[0], topics.shape[1]\n",
    "  for i in range(K):\n",
    "    topic = topics[i]\n",
    "    for j1 in range(len(topic) - 1):\n",
    "      for j2 in range(j1+1, len(topic)):\n",
    "        score += coherence_single(topic[j1], topic[j2], W) \n",
    "  return score / (K * V * (V-1) / 2)\n",
    "\n",
    "# prints top n most probable words in each topic of the model\n",
    "def print_topics(model, n, vocab):\n",
    "  beta = model.beta.softmax(dim=1).cpu().detach().numpy()\n",
    "  topn = np.argsort(beta, axis=1)[:, -n:]\n",
    "  for i in range(model.K):\n",
    "      print(f\"Topic {i}: eta = {model.eta[i]}\\n {vocab[topn[i]]}\")\n",
    "\n",
    "def s_term_normal(y_batch, gamma_batch, eta, delta, M):\n",
    "  h =  -0.5 * M * delta.log() - (y_batch ** 2).sum() / (2 * delta)         \n",
    "  g0 = gamma_batch.sum(dim=1, keepdim=True)\n",
    "  g = gamma_batch / g0\n",
    "  outer = contract('mi,mj->mij', g, g, backend='torch')\n",
    "  EXtX = (-outer / (g0.unsqueeze(2) + 1) + outer).sum(dim=0) + \\\n",
    "    torch.diag((g / (g0 + 1)).sum(dim=0))\n",
    "  EX = g      \n",
    "  first =  contract('m,k,mk->', y_batch, eta, EX, backend='torch') \n",
    "  second = contract('k,kq,q->', eta, EXtX, eta, backend='torch')     \n",
    "  s_term = h + (2 * first - second) / (2 * delta)\n",
    "  return s_term\n",
    "\n",
    "def s_term_bernoulli(y_batch, gamma_batch, eta):\n",
    "  g0 = gamma_batch.sum(dim=1, keepdim=True)\n",
    "  g = gamma_batch / g0\n",
    "  probs = contract('mk,k->m', g, eta, backend='torch').sigmoid()\n",
    "  # to prevent overflows in log\n",
    "  probs_cpy = probs\n",
    "  if probs.min() <= 0:\n",
    "    c = probs.min().detach()\n",
    "    probs = probs - c + self.epsilon\n",
    "  s_term1 = (y_batch * probs.log()).sum()  \n",
    "  probs = probs_cpy\n",
    "  if probs.max() >= 1:\n",
    "    c = probs.max().detach()\n",
    "    probs = probs - (c - 1) - self.epsilon\n",
    "  s_term2 = ((1-y_batch) * (1-probs).log()).sum()\n",
    "  s_term = s_term1 + s_term2\n",
    "  return s_term\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "LU8EEu3_-8Rm"
   },
   "outputs": [],
   "source": [
    "#@title Data Manager\n",
    "\n",
    "# Adjust this as necessary\n",
    "# DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/research/\"\n",
    "DATA_PATH = \"\"\n",
    "\n",
    "def save_dict(d, name):  \n",
    "  file = open(name,'wb')\n",
    "  pickle.dump(d, file)\n",
    "\n",
    "def load_dict(name):\n",
    "  file = open(name,'rb')\n",
    "  d = pickle.load(file)\n",
    "  return d\n",
    "\n",
    "# will need to upload this to your drive if using colab\n",
    "def load_Pang_Lee():\n",
    "  return load_dict(DATA_PATH + \"datasets/Pang_Lee\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ZDWsZ8wxAqYM"
   },
   "outputs": [],
   "source": [
    "#@title sLDA \n",
    "class sLDA(nn.Module):\n",
    "  \"\"\"\n",
    "  Implementation based on McAuliffe and Blei Supervised Topic Models\n",
    "  (https://arxiv.org/pdf/1003.0783.pdf).\n",
    "  Optimized with SGD on ELBO instead of coordinate ascent.\n",
    "  Constrained parameters stored in transformed way to allow\n",
    "  unconstrained gradient updates.\n",
    "  \"\"\"\n",
    "  def __init__(self, K, V, M, M_val, alpha_fixed=None, device=device): \n",
    "    \"\"\"\n",
    "    Args: \n",
    "    K: # topics, \n",
    "    V: Vocab size, \n",
    "    M: # docs,\n",
    "    M_val: # val docs,\n",
    "    alpha_fixed: if true fix alpha\n",
    "    \"\"\"\n",
    "    super(sLDA, self).__init__()\n",
    "    self.name = 'slda'\n",
    "    self.K = K    \n",
    "    self.V = V\n",
    "    self.M = M\n",
    "    self.epsilon = 0.0000001\n",
    "    self.alpha_fixed = alpha_fixed\n",
    "   \n",
    "    \n",
    "\n",
    "    # model parameters\n",
    "    alpha = torch.ones(self.K).to(device) if self.alpha_fixed else \\\n",
    "      ds.Exponential(1).sample([self.K]) \n",
    "    # beta stored pre-softmax (over V)\n",
    "    beta = ds.Exponential(1).sample([self.K, self.V])\n",
    "    beta = beta / beta.sum(dim=1, keepdim=True)   \n",
    "    eta = ds.Normal(0,1).sample([self.K])\n",
    "    # delta stored pre-exponentiated\n",
    "    delta = ds.Normal(0,1).sample().abs()   \n",
    "    \n",
    "    # variational parameters\n",
    "    gamma = torch.ones((self.M, self.K))\n",
    "    gamma_val = torch.ones((M_val, self.K))\n",
    "    # phi stored pre-softmax (over K)\n",
    "    phi = torch.ones((self.M, self.K, self.V))\n",
    "    phi_val = torch.ones((M_val, self.K, self.V))\n",
    "    \n",
    "    self.alpha = alpha if self.alpha_fixed else nn.Parameter(alpha)  \n",
    "    self.beta = nn.Parameter(beta)\n",
    "    self.gamma = nn.Parameter(gamma)\n",
    "    self.phi = nn.Parameter(phi)   \n",
    "    self.eta = nn.Parameter(eta)\n",
    "    self.delta = nn.Parameter(delta)\n",
    "    self.phi_val = nn.Parameter(phi_val)\n",
    "    self.gamma_val = nn.Parameter(gamma_val)\n",
    "\n",
    "\n",
    "  def ELBO(self, W_batch, phi_batch, gamma_batch, y_batch, version='real'):\n",
    "    \"\"\"\n",
    "    Computes sLDA ELBO:\n",
    "    first_term: log p(theta|alpha)\n",
    "    second_term: log p(z|theta)\n",
    "    third_term: log p(w|z,beta)\n",
    "    fourth_term: log q(theta|gamma)\n",
    "    fifth_term: log q(z|phi)\n",
    "    s_term: log p(y|theta)\n",
    "    \"\"\"\n",
    "    M = W_batch.shape[0]\n",
    "    ss = torch.digamma(gamma_batch) \\\n",
    "      - torch.digamma(gamma_batch.sum(dim=1, keepdim=True))\n",
    "    \n",
    "    # transform constrained parameters to be valid\n",
    "    phi = phi_batch.softmax(dim=1)\n",
    "    beta = self.beta.softmax(dim=1)\n",
    "    delta = self.delta.exp()\n",
    "\n",
    "    first_term = M * (torch.lgamma(self.alpha.sum()) \\\n",
    "      - torch.lgamma(self.alpha).sum()) \\\n",
    "      + contract('mk,k->', ss, self.alpha - 1, backend='torch')\n",
    "    \n",
    "    second_term = contract(\n",
    "      'mkv,mk,mv->', \n",
    "      phi, ss, W_batch, \n",
    "      backend='torch'\n",
    "    ) \n",
    "\n",
    "    third_term = contract(\n",
    "      'mkv,mv,kv->', \n",
    "      phi, W_batch, beta.log(), \n",
    "      backend='torch'\n",
    "    ) \n",
    "    \n",
    "    fourth_term = torch.lgamma(gamma_batch.sum(dim=1)).sum() \\\n",
    "      - torch.lgamma(gamma_batch).sum() \\\n",
    "      + contract('mk,mk->', ss, gamma_batch - 1, backend='torch')\n",
    "    \n",
    "    fifth_term = contract(\n",
    "      'mkv,mkv,mv->', \n",
    "      phi, phi.log(), W_batch, \n",
    "      backend='torch'\n",
    "    )\n",
    "\n",
    "    if version=='real':\n",
    "      s_term = s_term_normal(y_batch, gamma_batch, self.eta, delta, M)\n",
    "    else:\n",
    "      s_term = s_term_bernoulli(y_batch, gamma_batch, self.eta)\n",
    "\n",
    "    return first_term + second_term + third_term - \\\n",
    "      fourth_term - fifth_term + s_term\n",
    "\n",
    "  \n",
    "  # can also batch if needed, uses theta map as in pc-slda\n",
    "  # (http://proceedings.mlr.press/v84/hughes18a/hughes18a.pdf)\n",
    "  def pred(self, W, y=None):\n",
    "    theta_maps = self.theta_map(W)\n",
    "    preds = torch.mv(theta_maps, self.eta)\n",
    "    return theta_maps, preds\n",
    "\n",
    " \n",
    "  # calculate theta map with SGD on the posterior of theta\n",
    "  def theta_map(self, W, num_epochs = 500, lr = 0.005):      \n",
    "    theta_maps = torch.ones(\n",
    "      (W.shape[0], self.K), \n",
    "      requires_grad=True, \n",
    "      device=device\n",
    "    )\n",
    "    \n",
    "    opt = torch.optim.Adam([theta_maps], lr=lr)\n",
    "    for i in range(num_epochs):\n",
    "      opt.zero_grad()\n",
    "      score = self.theta_post(W, theta_maps)\n",
    "      loss = -1 * score\n",
    "      loss.sum().backward()\n",
    "      opt.step()\n",
    "    \n",
    "    return theta_maps.softmax(dim=1)\n",
    "        \n",
    "  \n",
    "  # calculate the posterior of theta\n",
    "  def theta_post(self, W, theta):\n",
    "    bl = contract(\n",
    "      'kv,mk->mv', \n",
    "      self.beta.softmax(dim=1), \n",
    "      theta.softmax(dim=1),\n",
    "      backend='torch'\n",
    "    )\n",
    "    out1 = contract('mv,mv->m', W, bl.log(), backend='torch') \n",
    "    out2 = torch.mv(theta.softmax(dim=1).log(), self.alpha - 1)\n",
    "    return out1 + out2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "7VNSfBvN_c-U"
   },
   "outputs": [],
   "source": [
    "#@title pf-sLDA\n",
    "class pfsLDA(nn.Module):\n",
    "\n",
    "  \"\"\"\n",
    "  Implementation based on Ren et. al. pf-sLDA\n",
    "  (https://arxiv.org/pdf/1910.05495.pdf).\n",
    "  Constrained parameters stored in transformed way to allow\n",
    "  unconstrained gradient updates.\n",
    "  \"\"\"\n",
    "  def __init__(self, K, V, M, M_val, p, alpha_fixed = None):\n",
    "    \"\"\"\n",
    "    Args: \n",
    "    K: # topics, \n",
    "    V: Vocab size, \n",
    "    M: # docs,\n",
    "    M_val: # val docs,\n",
    "    p : switch prior\n",
    "    alpha_fixed: if true fix alpha\n",
    "    \"\"\"\n",
    "    super(pfsLDA, self).__init__()\n",
    "    self.name = 'pfslda'\n",
    "    self.K = K      \n",
    "    self.V = V\n",
    "    self.M = M\n",
    "    self.M_val = M_val\n",
    "    self.epsilon = 0.0000001\n",
    "    self.alpha_fixed = alpha_fixed\n",
    "\n",
    "    # model parameters\n",
    "    alpha = torch.ones(self.K).to(device) if self.alpha_fixed else \\\n",
    "      ds.Exponential(1).sample([self.K])        \n",
    "    # beta stored pre-softmax (over V)\n",
    "    beta = ds.Exponential(1).sample([self.K, self.V])\n",
    "    beta = beta / beta.sum(dim=1, keepdim=True)   \n",
    "    # pi stored pre-softmax (over V)\n",
    "    pi = ds.Exponential(1).sample([self.V])\n",
    "    pi = pi / pi.sum()  \n",
    "    eta = ds.Normal(0,1).sample([self.K])\n",
    "    # delta stored pre-exponentiated\n",
    "    delta = ds.Normal(0,1).sample().abs() \n",
    "    \n",
    "    # variational parameters\n",
    "    gamma = torch.ones((self.M, self.K))\n",
    "    gamma_val = torch.ones(self.M_val, self.K)\n",
    "    # phi stored pre softmax (over K)\n",
    "    phi = torch.ones(self.M, self.K, self.V)\n",
    "    phi_val = torch.ones(self.M_val, self.K, self.V)\n",
    "    # varphi stored pre-sigmoid\n",
    "    varphi = torch.ones(self.V) * p \n",
    "\n",
    "    self.alpha = alpha if self.alpha_fixed else nn.Parameter(alpha)  \n",
    "    self.beta = nn.Parameter(beta)\n",
    "    self.gamma = nn.Parameter(gamma)\n",
    "    self.phi = nn.Parameter(phi)   \n",
    "    self.eta = nn.Parameter(eta)\n",
    "    self.delta = nn.Parameter(delta)\n",
    "    self.pi = nn.Parameter(pi)\n",
    "    self.varphi = nn.Parameter(varphi)    \n",
    "    self.phi_val = nn.Parameter(phi_val)\n",
    "    self.gamma_val = nn.Parameter(gamma_val)\n",
    "    self.p = p\n",
    "      \n",
    " \n",
    "  def ELBO(self, W_batch, phi_batch, gamma_batch, y_batch, version='real'):\n",
    "    \"\"\"\n",
    "    Computes pf-sLDA ELBO:\n",
    "    See appendix of https://arxiv.org/pdf/1910.05495.pdf for details\n",
    "    first_term: log p(theta|alpha)\n",
    "    second_term: log p(z|theta)\n",
    "    third_term: log p(w|z,beta)\n",
    "    fourth_term: log p(xi|p)\n",
    "    fifth_term: log q(theta|gamma)\n",
    "    sixth_term: log q(z|phi)\n",
    "    seventh_term: log q(xi|varphi)\n",
    "    s_term: log p(y|theta)\n",
    "    \"\"\"\n",
    "    M = W_batch.shape[0]\n",
    "    N_tot = W_batch.sum()\n",
    "    ss = torch.digamma(gamma_batch) - \\\n",
    "      torch.digamma(gamma_batch.sum(dim=1, keepdim=True))\n",
    "    \n",
    "    # transform constrained parameters to be valid\n",
    "    phi = phi_batch.softmax(dim=1)\n",
    "    beta = self.beta.softmax(dim=1)\n",
    "    pi = self.pi.softmax(dim=0)\n",
    "    varphi = self.varphi.sigmoid() \n",
    "    p = self.p.sigmoid()\n",
    "    delta = self.delta ** 2\n",
    "         \n",
    "    first_term = M \\\n",
    "      * (torch.lgamma(self.alpha.sum()) - torch.lgamma(self.alpha).sum()) \\\n",
    "      + contract('mk,k->', ss, self.alpha - 1, backend='torch')\n",
    "    \n",
    "    second_term = contract(\n",
    "      'mkv,mk,mv->', \n",
    "      phi, ss, W_batch, \n",
    "      backend='torch'\n",
    "    )  \n",
    "                      \n",
    "    third_term1 = contract(\n",
    "      'mkv,mv,kv,v->', \n",
    "      phi, W_batch, beta.log(), varphi, \n",
    "      backend='torch'\n",
    "    ) \n",
    "    third_term2 = contract(\n",
    "      'mv,v,v->', \n",
    "      W_batch, pi.log(), varphi, \n",
    "      backend='torch'\n",
    "    )\n",
    "    third_term3 = contract(\n",
    "      'mv,v->', \n",
    "      W_batch, pi.log(), \n",
    "      backend='torch'\n",
    "    )\n",
    "    third_term = third_term1 - third_term2 + third_term3\n",
    "   \n",
    "    fourth_term1 = contract(\n",
    "      'mv,v->', \n",
    "      W_batch, varphi, \n",
    "      backend='torch'\n",
    "    ) \n",
    "    fourth_term2 = contract(\n",
    "      'mv,v->', \n",
    "      W_batch, 1 - varphi, \n",
    "      backend='torch'\n",
    "    ) \n",
    "    fourth_term = p.log() * fourth_term1 + (1-p).log() * fourth_term2\n",
    "    \n",
    "    fifth_term = torch.lgamma(gamma_batch.sum(dim=1)).sum() - \\\n",
    "      torch.lgamma(gamma_batch).sum() + \\\n",
    "      contract('mk,mk->', ss, gamma_batch - 1, backend='torch')  \n",
    "\n",
    "    sixth_term = contract(\n",
    "      'mkv,mkv,mv->', \n",
    "      phi, phi.log(), W_batch, \n",
    "      backend='torch'\n",
    "    )\n",
    "              \n",
    "    orig_varphi = varphi\n",
    "    if varphi.min() <= 0:\n",
    "      c = varphi.min().detach()\n",
    "      varphi = varphi - c + self.epsilon\n",
    "    seventh_term1 = contract(\n",
    "      'mv,v,v->', \n",
    "      W_batch, varphi, varphi.log(), \n",
    "      backend='torch'\n",
    "    )\n",
    "    varphi = orig_varphi\n",
    "    if varphi.max() >= 1:\n",
    "      c = varphi.max().detach()\n",
    "      varphi = varphi - (c - 1) - self.epsilon\n",
    "    seventh_term2 = contract(\n",
    "      'mv,v,v->', \n",
    "      W_batch, 1 - varphi, (1 - varphi).log(), \n",
    "      backend='torch'\n",
    "    )\n",
    "    seventh_term = seventh_term1 + seventh_term2\n",
    "\n",
    "    if version=='real':\n",
    "      s_term = s_term_normal(y_batch, gamma_batch, self.eta, delta, M)\n",
    "    else:\n",
    "      s_term = s_term_bernoulli(y_batch, gamma_batch, self.eta)\n",
    "    \n",
    "    return first_term + second_term + third_term + fourth_term \\\n",
    "      - fifth_term - sixth_term - seventh_term + s_term\n",
    "\n",
    "  \n",
    "  # can also batch if needed, uses theta map as in pc-slda\n",
    "  # (http://proceedings.mlr.press/v84/hughes18a/hughes18a.pdf)  \n",
    "  def pred(self, W, y=None):\n",
    "    theta_maps = self.theta_map(W)\n",
    "    preds = torch.mv(theta_maps, self.eta)\n",
    "    return theta_maps, preds\n",
    "  \n",
    "  \n",
    "  # calculate theta map with SGD on the posterior of theta\n",
    "  def theta_map(self, W, num_epochs = 500, lr = 0.005):      \n",
    "    theta_maps = torch.ones(\n",
    "      (W.shape[0], self.K), \n",
    "      requires_grad = True, \n",
    "      device=device\n",
    "    )\n",
    "    opt = torch.optim.Adam([theta_maps], lr = lr)\n",
    "    for i in range(num_epochs):\n",
    "      opt.zero_grad()\n",
    "      score = self.theta_post(W, theta_maps)\n",
    "      loss = -1 * score\n",
    "      loss.sum().backward()\n",
    "      opt.step()\n",
    "    return theta_maps.softmax(dim=1)\n",
    "        \n",
    "  \n",
    "  # calculate the posterior of theta\n",
    "  def theta_post(self, W, theta):\n",
    "    bl = contract(\n",
    "      'kv,mk->mv', \n",
    "      self.beta.softmax(dim=1), \n",
    "      theta.softmax(dim=1), \n",
    "      backend='torch'\n",
    "    )\n",
    "    \n",
    "    out1 = contract(\n",
    "      'v,mv,mv->m', \n",
    "      self.varphi.sigmoid(), W, bl.log(), \n",
    "      backend='torch'\n",
    "    ) \n",
    "    out2 = torch.mv(theta.softmax(dim=1).log(), self.alpha)\n",
    "    \n",
    "    return out1 + out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "ftT6DJisAjI7"
   },
   "outputs": [],
   "source": [
    "#@title Training\n",
    "def fit(model, W, y, lr, lambd, num_epochs, batch_size, \n",
    "          check, version, W_val, y_val, device, y_thresh, c_thresh):     \n",
    "  \"\"\"\n",
    "  Args: \n",
    "  W: count data, \n",
    "  y: targets, \n",
    "  lr: initial learning rate\n",
    "  lambd: supervised task regularizer weight\n",
    "  \"\"\"     \n",
    "\n",
    "  print(f\"Training {model.name} on {device}.\")\n",
    "  opt = torch.optim.Adam(model.parameters(), lr = lr)\n",
    " \n",
    "  for i in range(num_epochs):\n",
    "    to_batch = [W, model.phi, model.gamma, y]    \n",
    "    batches = batchify(to_batch, batch_size)\n",
    "    W_b, phi_b, gamma_b, y_b = batches[0], batches[1], batches[2], batches[3]\n",
    "    tot = 0\n",
    "    for j in range(len(W_b)):\n",
    "      opt.zero_grad()    \n",
    "      elbo = model.ELBO(W_b[j].to(device), phi_b[j], gamma_b[j], y_b[j].to(device), version=version) \n",
    "      tot += elbo.item()\n",
    "      loss = -1 * elbo + lambd * (model.eta ** 2).sum()\n",
    "      loss.backward()\n",
    "      opt.step()\n",
    "\n",
    "    if i % check == 0:\n",
    "      val_yscore, c = calc_stats_and_print(\n",
    "        model, W, W_val.to(device), y_val.to(device), \n",
    "        tot / W.sum(), i,  version\n",
    "      )\n",
    "      \n",
    "      save = False\n",
    "      if (y_thresh and val_yscore < y_thresh) or (c_thresh and c > c_thresh):\n",
    "         save = True\n",
    "      if save:\n",
    "         path = DATA_PATH + f\"{model.name}_ y{val_yscore:.2f}_c{c:.2f}.pt\"\n",
    "         torch.save(model.state_dict(), path)\n",
    "    \n",
    "  # save last model if no thresholds   \n",
    "  if not y_thresh and not c_thresh:\n",
    "    val_yscore, c = calc_stats_and_print(\n",
    "      model, W, W_val.to(device), y_val.to(device), \n",
    "      tot / W.sum(), num_epochs,  version\n",
    "    )\n",
    "    path = DATA_PATH + f\"{model.name}_ y{val_yscore:.2f}_c{c:.2f}.pt\"\n",
    "    torch.save(model.state_dict(), path)\n",
    "\n",
    "  return \n",
    "\n",
    "def yscore(model, W, y, version='real'):\n",
    "  _, preds = model.pred(W)\n",
    "  if version == 'real':\n",
    "    score = ((preds - y) ** 2).mean().sqrt()\n",
    "  else:\n",
    "    probs = preds.sigmoid().cpu().detach().numpy()\n",
    "    score = auc(y.cpu().detach().numpy(), probs)\n",
    "  return score\n",
    "  \n",
    "def calc_stats_and_print(model, W, W_val, y_val, elbo, i, version):\n",
    "  val_yscore = yscore(model, W_val, y_val, version=version)  \n",
    "  beta = model.beta.softmax(dim=1).cpu().detach().numpy()\n",
    "  topk = np.argsort(beta, axis=1)[:, -50:]\n",
    "  c = coherence(topk, W)      \n",
    "  print(f\"Iter: {i}\")\n",
    "  print(f\"ELBO: {elbo}\")\n",
    "  print(f\"Val yscore: {val_yscore}\")\n",
    "  print(f\"Coherence: {c}\\n\")\n",
    "  return val_yscore, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "colab": {},
    "colab_type": "code",
    "id": "Cp574jOvA5EY"
   },
   "outputs": [],
   "source": [
    "#@title Main\n",
    "def main(args):\n",
    "  device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "  if args['K'] < 1:\n",
    "    raise ValueError('Invalid number of topics specified.')\n",
    "  \n",
    "  p = args['p']\n",
    "  if p > 1 or p < 0:\n",
    "    raise ValueError('Invalid switch prior p.')\n",
    "  p = torch.tensor(p).to(device)\n",
    "  p = torch.log(p / (1 - p))\n",
    "\n",
    "  d = load_Pang_Lee()\n",
    "  W = d['W']\n",
    "  W_val = d['W_val']\n",
    "  y = d['y']\n",
    "  y_val = d['y_val']  \n",
    "  W_test = d['W_test']\n",
    "  y_test = d['y_test']\n",
    "  vocab = d['vocab']\n",
    "  version = 'real'\n",
    "\n",
    "  V = W.shape[1]\n",
    "  M = W.shape[0]\n",
    "  M_val = W_val.shape[0]\n",
    "\n",
    "  if args['model'] == 'slda':\n",
    "      model = sLDA(args['K'], V, M, M_val, alpha_fixed=args['alpha'])\n",
    "  elif args['model'] == 'pfslda':\n",
    "      model = pfsLDA(args['K'], V, M, M_val, p, alpha_fixed=args['alpha'])\n",
    "  model.to(device)\n",
    "\n",
    "  if 'path' in args:\n",
    "      state_dict = torch.load(args['path'], map_location = device)\n",
    "      model.load_state_dict(state_dict)\n",
    "\n",
    "  kwargs = {\n",
    "      'W' : W,\n",
    "      'y' : y, \n",
    "      'lr' : args['lr'], \n",
    "      'lambd' : args['lambd'],\n",
    "      'num_epochs' : args['num_epochs'], \n",
    "      'check' : args['check'], \n",
    "      'batch_size' : args['batch_size'], \n",
    "      'version' : version,\n",
    "      'W_val' : W_val,\n",
    "      'y_val' : y_val,\n",
    "      'device' : device,\n",
    "      'y_thresh' : args['y_thresh'],\n",
    "      'c_thresh' : args['c_thresh']\n",
    "  }\n",
    "\n",
    "  fit(model, **kwargs)\n",
    "  print_topics(model, 10, vocab)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6a2zZPuj__WZ"
   },
   "outputs": [],
   "source": [
    "args = {\n",
    "    'K' : 5,\n",
    "    'model' : 'pfslda',\n",
    "    'p' : 0.15,\n",
    "    'alpha' : True,\n",
    "    'lr' : 0.025, \n",
    "    'lambd' : 0,\n",
    "    'num_epochs' : 100, \n",
    "    'check' : 10, \n",
    "    'batch_size' : 100, \n",
    "    'y_thresh' : None,\n",
    "    'c_thresh' : None\n",
    "}\n",
    "\n",
    "model = main(args)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PredictionFocusedTopicModels.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
